{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b39f3ef",
   "metadata": {},
   "source": [
    "# Monday exercises I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f9ddf2",
   "metadata": {},
   "source": [
    "## High-dimensional learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b9e5c",
   "metadata": {},
   "source": [
    "## Curse of dimensionality\n",
    "Here we are looking at different ways of understanding how distances behave in high dimensions and how this differs from their behavior in low-dimensional space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f91f61",
   "metadata": {},
   "source": [
    "**Exercises 1** Consider the unit cube in $d$ dimensions, that is $[0,1]^d$ in $\\mathbb{R}^d$. Assume we have a dataset that is uniformly distributed in the cube.\n",
    "  - Consider a smaller cube inside of our cube with a sidelenght $0<s<1$. What does $s$ need to be in 50 dimensions cover $1\\%$ of the data? In general, if d is the dimension, what is the sidelength $s$ of a smaller cube that contains a proportion $p$ of the data? \n",
    "  - Create a figure that shows how the volume increases as the side length of a cube increases from $0$ to $1$ in dimensions $1, 2, 5, 10, 20, 50, 100$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d4b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Write it here\n",
    "# The volume of the small cube is ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b24b0d6",
   "metadata": {},
   "source": [
    "**Exercise 2.** Another way of understanding the curse of dimensionality is to consider the multivariate standard normal distribution. Intuition from lower dimensions suggests that most points of the multivariate normal are close to the origin and therefore have a relatively small norm. \n",
    "  - For a random sample of 1000 normally distributed data points in dimensions $1, 2, 5, 10, 20, 50, 100$, plot the distribution of the norms. \n",
    "  - Describe what you see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21ff313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a618af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: Write it here\n",
    "\n",
    "\n",
    "# We see that the norm increases with dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3425553",
   "metadata": {},
   "source": [
    "3. Yet another way of visualizing the curse of dimensionality is to look at distances between uniformly sampled data. The normalized distance in $d$ dimensions is the distance divided by square root of $d$. Because the maximum distance in d-dimensional hypercube is $\\sqrt{d}$, the normalized distance will be between 0 and 1.\n",
    "  - For dimensions $1, 2, 5, 10, 100, 1000, 10000, 100000$, sample $1000$ points uniformly at random in the unit cube. \n",
    "  - Compute the difference between the maximum and the minimum distance divided by the minimum distance. In other words, we want\n",
    "  $$\\frac{d_{\\max}-d_{\\min}}{d_{min}}$$\n",
    "  What do you expect this to be?\n",
    "  - Compute the mean normalized distances\n",
    "  $$\\frac{1}{\\sqrt{d}} \\cdot \\text{mean of all distances}$$\n",
    "  What do you expect this to be?\n",
    "  - How do you interpret the results? Hint: view the minimum distance $d_{\\min}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76da1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be helpful to inport\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc2d2ee",
   "metadata": {},
   "source": [
    "## Learning in high dimensions\n",
    "Here we look at how learning behavior changes in higher dimensions when compared to learning in low dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecc0e1d",
   "metadata": {},
   "source": [
    "### Exercise Part 1\n",
    "\n",
    "1. Create a simple dataset with $n=50$ data points, where $x$ is uniformly distributed between 0 and 10 and $y$ is given by $y=5\\sin(x) + 0.2x^2 + \\varepsilon $, where $\\varepsilon$ is standard normally distributed. \n",
    "2. Split the data into training, validation and testdata. Visualize the training data.\n",
    "3. Train 4 different models on this dataset. You can for example use the onces we imported from `sklearn` (you do not need to set up a neural net). Visualize the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31644d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These might be helpful\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18221bcc",
   "metadata": {},
   "source": [
    "### Exercise Part 2\n",
    "\n",
    "4. Randomly rotate your data $x$ in 100-dimensional space. Use for example `scipy.stats.special_ortho_group`. Repeat steps 2-3. \n",
    "5. Add noise to your original training data. Repeat steps 2-3. \n",
    "6. Randomly rotate your data $x$ in 100-dimensional space, then add noise to your training data. Repeat steps 2-3.\n",
    "7. Summarize your results. What have you found?\n",
    "8. Create another dataset with \\(n=50\\) data points, where $x_1, \\dots, x_{100}$ are uniformly distributed between 0 and 10 and y is given by $y=\\sum_{i=1}^{100} \\sin(a_i x_i) + b_i x_i^2 + \\varepsilon$, where \\(a_i, b_i\\) are uniformly distributed between 0 and 1 and \\(\\varepsilon\\) is standard normally distributed. Train the same models on this dataset and summarize the results. What have you found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c9dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might be helpful to get a random rotation in dim 100\n",
    "from scipy.stats import special_ortho_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Just rotation\n",
    "\n",
    "dim = 100\n",
    "\n",
    "rot = special_ortho_group.rvs(dim, random_state=0)\n",
    "\n",
    "def add_dim(x):\n",
    "    return np.hstack((x.reshape(-1, 1), np.zeros((len(x), dim-1))))\n",
    "\n",
    "def rotate(x):\n",
    "    return add_dim(x).dot(rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cd94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecaca7a",
   "metadata": {},
   "source": [
    "### Exercises Part 3\n",
    "\n",
    "8. Create another dataset with \\(n=50\\) data points in $\\mathbb{R}^{100}$, where $x[1], \\dots, x[100]$ are uniformly distributed between 0 and 10 and y is given by\n",
    "$$y=\\sum_{i=1}^{100} (\\sin(a_i x_i) + b_i x_i^2) + \\varepsilon$$\n",
    "where $a_i, b_i$ are uniformly distributed between 0 and 1 and $\\varepsilon$ is standard normally distributed. Train the same models on this dataset and summarize the results. What have you found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff02c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your solution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
