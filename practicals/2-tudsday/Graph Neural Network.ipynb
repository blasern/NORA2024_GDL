{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.separate import separate\n",
    "from torch_geometric.datasets import Planetoid, ZINC\n",
    "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for node prediction\n",
    "\n",
    "A now classic dataset for GNNs (one whose use is also discouraged within some circles as it is very easy to overfit on), Cora is a nice small dataset to start looking at GNNs. There are many variations of the Cora dataset originally presented in \"Automating the Construction of Internet Portals with Machine Learning\" by McCallum et al. (https://link.springer.com/article/10.1023/A:1009953814988).\n",
    "\n",
    "We will use the Cora dataset variant as presented in “FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling” (https://arxiv.org/abs/1801.10247). It describes a citation network of 2708 papers and our task is classify each paper into one of 7 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "data_dir = \"./cora_data_full\"\n",
    "try: \n",
    "    os.mkdir(data_dir)\n",
    "except FileExistsError:\n",
    "    print(data_dir, 'exists')\n",
    "    \n",
    "dataset = Planetoid(data_dir, 'Cora', split=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, validation and test nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "train_x = data.x[data.train_mask]\n",
    "train_y = data.y[data.train_mask]\n",
    "\n",
    "valid_x = data.x[data.val_mask]\n",
    "valid_y = data.y[data.val_mask]\n",
    "\n",
    "test_x = data.x[data.test_mask]\n",
    "test_y = data.y[data.test_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"A simple feed forward neural network with no hidden layers\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = FFNN(input_dim=train_x.shape[-1], output_dim=7)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "val_accuracy = {key: 0 for key in range(n_epochs)}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    y_hat = model(train_x)\n",
    "    loss = F.cross_entropy(y_hat, train_y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(valid_x)\n",
    "    y_hat = y_hat.data.max(1)[1]\n",
    "    num_correct = y_hat.eq(valid_y.data).sum()\n",
    "    num_total = len(valid_y)\n",
    "    val_accuracy[epoch] = 100.0 * (num_correct.detach().numpy()/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=range(n_epochs), y=np.array(list(val_accuracy.values())))\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation accuracy (in %)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself graph neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic equation for convolutional GNN is\n",
    "$$\n",
    "\\mathbf{h_i} = \\phi \\big(\\mathbf{x_i}, \\oplus_{j \\in \\mathcal{N}_i} c_{i,j} \\psi (\\mathbf{x_j}) \\big).\n",
    "$$\n",
    "Rewrite this in terms of adjacency matrix $\\mathbf{A}$ and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCLayer(nn.Module):\n",
    "    \"\"\"Graph convolution layer\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.A = A\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### TODO\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = GCLayer(input_dim, output_dim, A)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency matrix\n",
    "adj = to_dense_adj(data.edge_index)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trin the your graph neural network and plot the validation accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself graph neural network - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic equation for convolutional GNN is\n",
    "$$\n",
    "\\mathbf{h_i} = \\phi \\big(\\mathbf{x_i}, \\oplus_{j \\in \\mathcal{N}_i} c_{i,j} \\psi (\\mathbf{x_j}) \\big).\n",
    "$$\n",
    "\n",
    "Specific implementations often differ slightly from this form. For example Kipf and Welling's \"Semi-Supervised Classification with Graph Convolutional Networks\" (https://arxiv.org/abs/1609.02907) employs a symmetric normalisation for the convolution coefficients with a re-normalisation to tackle exploding parameters. Here\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\sigma \\big( \\mathbf{\\tilde{D}}^{-\\frac{1}{2}} \\mathbf{\\tilde{A}} \\mathbf{\\tilde{D}}^{-\\frac{1}{2}} \\mathbf{X} \\mathbf{W} \\big),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\tilde{A}} = \\mathbf{A} + \\mathbf{I}$ and $\\mathbf{\\tilde{D}}$ is the degree matrix of $\\mathbf{\\tilde{A}}$.\n",
    "\n",
    "Is the behaviour of the generic, convolutional GNN layer the same as Kipf and Welling's GCN? Implement this version.  How does the performance compare to the generic version above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard graph neural network layers\n",
    "\n",
    "Have a look at some standard implementations of graph neural network layers, for example `GCNConv`, `GATConv` and `TransformerConv`. \n",
    "\n",
    "Note that these do not use the adjacency matrix, but the edge list (`data.edge_index`). \n",
    "\n",
    "Implement the models and show their validation accuracies during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for graph-level prediction\n",
    "\n",
    "For this, we will use the ZINC dataset for graph-regression. It contains about 12 000 molecular graphs with up to 38 nodes each and the task is to predict for each molecule the solubility (a scalar number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "data_dir = \"./zinc_data\"\n",
    "try: \n",
    "    os.mkdir(data_dir)\n",
    "except FileExistsError:\n",
    "    print(data_dir, 'exists')\n",
    "\n",
    "train_zinc_dataset = ZINC(root=data_dir, split='train', subset=True)\n",
    "val_zinc_dataset = ZINC(root=data_dir, split='val', subset=True)\n",
    "test_zinc_dataset = ZINC(root=data_dir, split='test', subset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN for graph-level prediction\n",
    "\n",
    "Write a GNN for graph-level prediction and use it to predict the molecule solubility. \n",
    "\n",
    "Note: Using batch-sizes larger than `1` requires that you aggregate over the individual graphs, not the entire batch. To do this, I have used the following `extract`-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(x, idx, slices):\n",
    "    \"\"\" Extract x for graph with index idx\n",
    "    Args: \n",
    "        x: data\n",
    "        idx: index of graph in batch\n",
    "        slices: indices where graphs start and end, e.g. batch.ptr\n",
    "    \"\"\"\n",
    "    start, end = int(slices[idx]), int(slices[idx + 1])\n",
    "    return x.narrow(0, start, end-start).sum(axis=0).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdl] *",
   "language": "python",
   "name": "conda-env-gdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
