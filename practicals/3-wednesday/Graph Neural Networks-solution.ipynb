{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.separate import separate\n",
    "from torch_geometric.datasets import Planetoid, ZINC\n",
    "from torch_geometric.loader import DataLoader, LinkNeighborLoader\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for node prediction\n",
    "\n",
    "A now classic dataset for GNNs (one whose use is also discouraged within some circles as it is very easy to overfit on), Cora is a nice small dataset to start looking at GNNs. There are many variations of the Cora dataset originally presented in \"Automating the Construction of Internet Portals with Machine Learning\" by McCallum et al. (https://link.springer.com/article/10.1023/A:1009953814988).\n",
    "\n",
    "We will use the Cora dataset variant as presented in “FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling” (https://arxiv.org/abs/1801.10247). It describes a citation network of 2708 papers and our task is classify each paper into one of 7 different categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "data_dir = \"./cora_data_full\"\n",
    "try: \n",
    "    os.mkdir(data_dir)\n",
    "except FileExistsError:\n",
    "    print(data_dir, 'exists')\n",
    "    \n",
    "dataset = Planetoid(data_dir, 'Cora', split=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data is valid\n",
    "data = dataset[0]\n",
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data\n",
    "print('Size:', len(dataset))\n",
    "print('Classes:', dataset.num_classes)\n",
    "print('Node features:', dataset.num_node_features)\n",
    "print('Edge features:', dataset.num_edge_features)\n",
    "\n",
    "print('Total number of nodes:', data.num_nodes)\n",
    "print('Total number of edges:', data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train nodes:', torch.sum(data.train_mask))\n",
    "print('Val nodes:', torch.sum(data.val_mask))\n",
    "print('Test nodes:', torch.sum(data.test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "train_x = data.x[data.train_mask]\n",
    "train_y = data.y[data.train_mask]\n",
    "\n",
    "valid_x = data.x[data.val_mask]\n",
    "valid_y = data.y[data.val_mask]\n",
    "\n",
    "test_x = data.x[data.test_mask]\n",
    "test_y = data.y[data.test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train shape x: {train_x.shape}, y: {train_y.shape}\")\n",
    "print(f\"Val shape x: {valid_x.shape}, y: {valid_y.shape}\")\n",
    "print(f\"Test shape x: {test_x.shape}, y: {test_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"A simple feed forward neural network with no hidden layers\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = FFNN(input_dim=train_x.shape[-1], output_dim=7)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "val_accuracy = {key: 0 for key in range(n_epochs)}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    y_hat = model(train_x)\n",
    "    loss = F.cross_entropy(y_hat, train_y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(valid_x)\n",
    "    y_hat = y_hat.data.max(1)[1]\n",
    "    num_correct = y_hat.eq(valid_y.data).sum()\n",
    "    num_total = len(valid_y)\n",
    "    val_accuracy[epoch] = 100.0 * (num_correct.detach().numpy()/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=range(n_epochs), y=np.array(list(val_accuracy.values())))\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation accuracy (in %)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself graph neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic equation for convolutional GNN is\n",
    "$$\n",
    "\\mathbf{h_i} = \\phi \\big(\\mathbf{x_i}, \\oplus_{j \\in \\mathcal{N}_i} c_{i,j} \\psi (\\mathbf{x_j}) \\big).\n",
    "$$\n",
    "Rewrite this in terms of adjacency matrix $\\mathbf{A}$ and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"GCN layer\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.A = A\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.matmul(self.A, x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model using different layers\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = GCNLayer(input_dim, output_dim, A)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjacency matrix\n",
    "adj = to_dense_adj(data.edge_index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = GNN(input_dim=train_x.shape[-1], output_dim=7, A=adj)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "val_accuracy = {key: 0 for key in range(n_epochs)}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    y_hat = model(data.x)[data.train_mask]\n",
    "    loss = F.cross_entropy(y_hat, train_y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(data.x)[data.val_mask]\n",
    "    y_hat = y_hat.data.max(1)[1]\n",
    "    num_correct = y_hat.eq(valid_y.data).sum()\n",
    "    num_total = len(valid_y)\n",
    "    val_accuracy[epoch] = 100.0 * (num_correct.detach().numpy()/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(list(val_accuracy.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=range(n_epochs), y=np.array(list(val_accuracy.values())))\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation accuracy (in %)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do it yourself graph neural network - normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generic equation for convolutional GNN is\n",
    "$$\n",
    "\\mathbf{h_i} = \\phi \\big(\\mathbf{x_i}, \\oplus_{j \\in \\mathcal{N}_i} c_{i,j} \\psi (\\mathbf{x_j}) \\big).\n",
    "$$\n",
    "\n",
    "Specific implementations often differ slightly from this form. For example Kipf and Welling's \"Semi-Supervised Classification with Graph Convolutional Networks\" (https://arxiv.org/abs/1609.02907) employs a symmetric normalisation for the convolution coefficients with a re-normalisation to tackle exploding parameters. Here\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\sigma \\big( \\mathbf{\\tilde{D}}^{-\\frac{1}{2}} \\mathbf{\\tilde{A}} \\mathbf{\\tilde{D}}^{-\\frac{1}{2}} \\mathbf{X} \\mathbf{W} \\big),\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\tilde{A}} = \\mathbf{A} + \\mathbf{I}$ and $\\mathbf{\\tilde{D}}$ is the degree matrix of $\\mathbf{\\tilde{A}}$.\n",
    "\n",
    "Is the behaviour of the generic, convolutional GNN layer the same as Kipf and Welling's GCN? Implement this version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"GCN layer\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.A = A\n",
    "\n",
    "        self.adj_norm = adj + torch.eye(A.shape[0])\n",
    "        self.degree = torch.inverse(torch.sqrt(torch.diag(adj.sum(axis=1))))\n",
    "        self.adj_norm = torch.matmul(torch.matmul(self.degree, self.adj_norm), self.degree)\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = torch.matmul(self.adj_norm, x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model using different layers\n",
    "\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, A):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = GCNLayer(input_dim, output_dim, A)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_dense_adj(data.edge_index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "model = GNN(input_dim=train_x.shape[-1], output_dim=7, A=adj)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "val_accuracy = {key: 0 for key in range(n_epochs)}\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "    y_hat = model(data.x)[data.train_mask]\n",
    "    loss = F.cross_entropy(y_hat, train_y)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(data.x)[data.val_mask]\n",
    "    y_hat = y_hat.data.max(1)[1]\n",
    "    num_correct = y_hat.eq(valid_y.data).sum()\n",
    "    num_total = len(valid_y)\n",
    "    val_accuracy[epoch] = 100.0 * (num_correct.detach().numpy()/num_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(list(val_accuracy.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=range(n_epochs), y=np.array(list(val_accuracy.values())))\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation accuracy (in %)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Existing graph neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model using different layers\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output softmax distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim, layer):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer = layer(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "\n",
    "layers = {'GCNConv': GCNConv, 'GATConv': GATConv, 'TransformerConv': TransformerConv}\n",
    "\n",
    "model_accuracies = {}\n",
    "for key, layer in layers.items():\n",
    "    model = GNN(input_dim=train_x.shape[-1], output_dim=7, layer=layer)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    val_accuracy = {key: 0 for key in range(n_epochs)}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        y_hat = model(data.x)[data.train_mask]\n",
    "        loss = F.cross_entropy(y_hat, train_y)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        model.eval()\n",
    "        y_hat = model(data.x)[data.val_mask]\n",
    "        y_hat = y_hat.data.max(1)[1]\n",
    "        num_correct = y_hat.eq(valid_y.data).sum()\n",
    "        num_total = len(valid_y)\n",
    "        val_accuracy[epoch] = 100.0 * (num_correct.detach().numpy()/num_total)\n",
    "        \n",
    "    model_accuracies[key] = val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = pd.DataFrame(np.array([np.array(list(acc.values())) for acc in model_accuracies.values()]).transpose(), \n",
    "                       columns=layers.keys())\n",
    "val_acc.loc[:, 'epoch'] = range(n_epochs)\n",
    "\n",
    "print('Maximal validation accuracies:\\n', val_acc.set_index('epoch').max(axis=0), sep='')\n",
    "\n",
    "val_acc = val_acc.melt(id_vars='epoch', value_name='validation accuracy', var_name='layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(data_frame=val_acc, x='epoch', y='validation accuracy', color='layer')\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation accuracy (in %)\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for graph-level prediction\n",
    "\n",
    "For this, we will use the ZINC dataset for graph-regression. It contains about 12 000 molecular graphs with up to 38 nodes each and the task is to predict for each molecule the solubility (a scalar number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset:\n",
    "data_dir = \"./zinc_data\"\n",
    "try: \n",
    "    os.mkdir(data_dir)\n",
    "except FileExistsError:\n",
    "    print(data_dir, 'exists')\n",
    "\n",
    "train_zinc_dataset = ZINC(root=data_dir, split='train', subset=True)\n",
    "val_zinc_dataset = ZINC(root=data_dir, split='val', subset=True)\n",
    "test_zinc_dataset = ZINC(root=data_dir, split='test', subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data description\n",
    "print(f\"\\nTrain examples: {len(train_zinc_dataset)}\")\n",
    "print(f\"Val examples: {len(val_zinc_dataset)}\")\n",
    "print(f\"Test examples: {len(test_zinc_dataset)}\\n\")\n",
    "\n",
    "# Description of first graph\n",
    "one_graph = train_zinc_dataset[0]\n",
    "print(f\"First graph contains {one_graph.x.shape[0]} nodes, each characterised by {one_graph.x.shape[1]} features\")\n",
    "print(f\"Graph labels have shape: {one_graph.y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN for graph-level prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(x, idx, slices):\n",
    "    start, end = int(slices[idx]), int(slices[idx + 1])\n",
    "    return x.narrow(0, start, end-start).sum(axis=0).reshape(1, -1)\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    \"\"\"GNN model using different layers\n",
    "    Args:\n",
    "        input_dim (int): Dimensionality of the input feature vectors\n",
    "        output_dim (int): Dimensionality of the output distribution\n",
    "        A (torch.Tensor): 2-D adjacency matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, mid_dim, output_dim, layer):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.graph_layer = layer(input_dim, mid_dim)\n",
    "        self.linear = nn.Linear(mid_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, ptr):\n",
    "        x = self.graph_layer(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = torch.concat([extract(x, idx, ptr) for idx in range(len(ptr)-1)], axis=0)\n",
    "        x = self.linear(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        return x.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(train_zinc_dataset, batch_size=16)\n",
    "valloader = DataLoader(val_zinc_dataset, batch_size=16)\n",
    "y_val = np.array([graph.y.detach().numpy() for graph in val_zinc_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GNN(input_dim=1, mid_dim=8, output_dim=1, layer=GCNConv).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "\n",
    "model_rmses = {}\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs): \n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        data = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.float(), data.edge_index, data.ptr)\n",
    "        loss = F.mse_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        outputs = np.hstack([np.array(model(batch.x.float(), batch.edge_index, batch.ptr)) for batch in valloader]).reshape(-1, 1)\n",
    "        model_rmses[epoch] = mean_squared_error(y_val, outputs, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=range(n_epochs), y=np.array(list(model_rmses.values())))\n",
    "\n",
    "fig.update_layout(template=\"plotly_white\", \n",
    "                  xaxis_title=\"epoch\",\n",
    "                  yaxis_title=\"validation RMSE\")\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gdl] *",
   "language": "python",
   "name": "conda-env-gdl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
